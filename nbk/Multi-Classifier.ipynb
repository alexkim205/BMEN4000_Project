{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.enable_eager_execution()\n",
    "save_path = '/home/final/data/cache/{}.bottle_necks.labels.paths.npz'\n",
    "plot_path = '/home/final/data/plots/multi_classifier'\n",
    "\n",
    "device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Xception Classifier Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionClassifier(tf.keras.Model):\n",
    "    def __init__(self, n_classes, n_layers):\n",
    "        self.n_layers = n_layers\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        super(XceptionClassifier, self).__init__()\n",
    "#         self.xception_layers = Xception(include_top=False, weights='imagenet', input_shape=(200,200,3))\n",
    "#         self.pooling_layer = GlobalAveragePooling2D(data_format='channels_last')\n",
    "        if n_layers  == 3:\n",
    "            self.dense_layer1 = tf.keras.layers.Dense(units=1024, activation='relu')\n",
    "            self.dense_layer2 = tf.keras.layers.Dense(units=512, activation='relu')\n",
    "            self.dense_layer3 = tf.keras.layers.Dense(units=self.n_classes)\n",
    "        if n_layers  == 6:\n",
    "            self.dense_layer1 = tf.keras.layers.Dense(units=1024, activation='relu')\n",
    "            self.dense_layer2 = tf.keras.layers.Dense(units=512, activation='relu')\n",
    "            self.dense_layer3 = tf.keras.layers.Dense(units=256, activation='relu')\n",
    "            self.dense_layer4 = tf.keras.layers.Dense(units=128, activation='relu')\n",
    "            self.dense_layer5 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "            self.dense_layer6 = tf.keras.layers.Dense(units=self.n_classes)\n",
    "        if n_layers  == 9:\n",
    "            self.dense_layer1 = tf.keras.layers.Dense(units=1024, activation='relu')\n",
    "            self.dense_layer2 = tf.keras.layers.Dense(units=512, activation='relu')\n",
    "            self.dense_layer3 = tf.keras.layers.Dense(units=256, activation='relu')\n",
    "            self.dense_layer4 = tf.keras.layers.Dense(units=128, activation='relu')\n",
    "            self.dense_layer5 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "            self.dense_layer6 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "            self.dense_layer7 = tf.keras.layers.Dense(units=16, activation='relu')\n",
    "            self.dense_layer8 = tf.keras.layers.Dense(units=8, activation='relu')\n",
    "            self.dense_layer9 = tf.keras.layers.Dense(units=self.n_classes)\n",
    "            \n",
    "    def call(self, inputs):\n",
    "#         xception = self.xception_layers(inputs)\n",
    "#         pooling = self.pooling_layer(xception)\n",
    "        if self.n_layers == 3:\n",
    "            result = self.dense_layer1(inputs)\n",
    "            result = self.dense_layer2(result)\n",
    "            result = self.dense_layer3(result)\n",
    "        if self.n_layers == 6:\n",
    "            result = self.dense_layer1(inputs)\n",
    "            result = self.dense_layer2(result)\n",
    "            result = self.dense_layer3(result)\n",
    "            result = self.dense_layer4(result)\n",
    "            result = self.dense_layer5(result)\n",
    "            result = self.dense_layer6(result)\n",
    "        if self.n_layers == 9:\n",
    "            result = self.dense_layer1(inputs)\n",
    "            result = self.dense_layer2(result)\n",
    "            result = self.dense_layer3(result)\n",
    "            result = self.dense_layer4(result)\n",
    "            result = self.dense_layer5(result)\n",
    "            result = self.dense_layer6(result)\n",
    "            result = self.dense_layer7(result)\n",
    "            result = self.dense_layer8(result)\n",
    "            result = self.dense_layer9(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(classifier, images, labels):\n",
    "    logits = classifier(images)\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "def calculate_f1(classifier, images, labels):\n",
    "    logits = tf.argmax(classifier(images), axis=1)\n",
    "    TP = tf.count_nonzero(logits * labels)\n",
    "    TN = tf.count_nonzero((logits - 1) * (labels - 1))\n",
    "    FP = tf.count_nonzero(logits * (labels - 1))\n",
    "    FN = tf.count_nonzero((logits - 1) * labels)\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def train(train_dataset, valid_dataset,\n",
    "          learning_rate, batch_size, n_epochs, n_layers, n_classes,\n",
    "          save_plot_fname,\n",
    "          plot_graphs=False, save_graphs=True\n",
    "         ):\n",
    "    \n",
    "    def _plot_loss(train, val, plot, save):\n",
    "        plt.figure(figsize=(9,6))\n",
    "        plt.plot(train, label='Train Loss')\n",
    "        plt.plot(val, label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss History')\n",
    "        if save: \n",
    "            plt.savefig(\"{}/{}_{}.png\".format(plot_path, save_plot_fname, \"loss\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "        if plot: \n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    def _plot_acc(train, val, plot, save):\n",
    "        plt.figure(figsize=(9,6))\n",
    "        plt.plot(train, label='Train Accuracy')\n",
    "        plt.plot(val, label='Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy History')\n",
    "        if save: \n",
    "            plt.savefig(\"{}/{}_{}.png\".format(plot_path, save_plot_fname, \"accr\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "        if plot: \n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    def _plot_f1(train, val, plot, save):\n",
    "        plt.figure(figsize=(9,6))\n",
    "        plt.plot(train, label='Train F1')\n",
    "        plt.plot(val, label='Validation F1')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 Score History')\n",
    "        if save: \n",
    "            plt.savefig(\"{}/{}_{}.png\".format(plot_path, save_plot_fname, \"f1sc\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "        if plot: \n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    \n",
    "    x_classifier = XceptionClassifier(n_classes=n_classes, n_layers=n_layers)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate) \n",
    "\n",
    "    # Performance Metrics\n",
    "    # - F1 Score\n",
    "    train_F1_history = []\n",
    "    val_F1_history = []\n",
    "    # - Loss\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    # - Accuracy\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    with tf.device(device):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            epoch_loss_avg = tfe.metrics.Mean()\n",
    "            epoch_acc = tfe.metrics.Accuracy()\n",
    "            epoch_f1 = tfe.metrics.Mean()\n",
    "            for batch, (tr_img, tr_lbl) in enumerate(train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Compute logits, logits are the domain/input to softmax\n",
    "                    tr_loss = calculate_loss(x_classifier, tr_img, tr_lbl)\n",
    "                \n",
    "                # Compute gradient and apply gradients\n",
    "                grads = tape.gradient(tr_loss, x_classifier.variables)\n",
    "                optimizer.apply_gradients(zip(grads, x_classifier.variables),\n",
    "                                          global_step=tf.train.get_or_create_global_step())\n",
    "                \n",
    "                # Add current batch metrics\n",
    "                epoch_loss_avg(tr_loss) # calculate avg epoch loss \n",
    "                epoch_acc(tf.argmax(x_classifier(tr_img), axis=1), tr_lbl)\n",
    "                epoch_f1(calculate_f1(x_classifier, tr_img, tr_lbl)) # calculate avg epoch f1 score \n",
    "                \n",
    "                if batch % 10 == 0:\n",
    "                    print('\\rEpoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, tr_loss.numpy()), end='')\n",
    "            # Add train loss, accuracy, and f1 for epoch\n",
    "            train_loss_history.append(epoch_loss_avg.result())\n",
    "            train_acc_history.append(epoch_acc.result())\n",
    "            train_F1_history.append(epoch_f1.result())\n",
    "\n",
    "            \n",
    "            # Run validation loop at end of each epoch\n",
    "            val_loss_avg = tfe.metrics.Mean()\n",
    "            val_acc = tfe.metrics.Accuracy()\n",
    "            val_f1 = tfe.metrics.Mean()\n",
    "            with tf.device(device):\n",
    "                for batch, (val_img, val_lbl) in enumerate(valid_dataset):\n",
    "                    # Compute validation metrics\n",
    "                    val_loss = calculate_loss(x_classifier, val_img, val_lbl)\n",
    "                    val_loss_avg(val_loss) # Add current batch loss\n",
    "                    val_acc(tf.argmax(x_classifier(val_img), axis=1), val_lbl)\n",
    "                    val_f1(calculate_f1(x_classifier, val_img, val_lbl))\n",
    "                \n",
    "                val_loss_history.append(val_loss_avg.result())\n",
    "                val_acc_history.append(val_acc.result())\n",
    "                val_F1_history.append(val_f1.result())\n",
    "            \n",
    "            # Print progress of epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"\\rEpoch {:03d}: Train F1:{:.3f}, Train Loss:{:.3f}, Train Acc:{:.3%}, Val F1:{:.3%}, Val Loss: {:.3f}, Val Acc: {:.3%}\".format(\n",
    "                    epoch, epoch_f1.result(), epoch_loss_avg.result(), epoch_acc.result(), \n",
    "                    val_f1.result(), val_loss_avg.result(), val_acc.result()))\n",
    "    \n",
    "    _plot_loss(train_loss_history, val_loss_history, plot_graphs, save_graphs)\n",
    "    _plot_acc(train_acc_history, val_acc_history, plot_graphs, save_graphs)\n",
    "    _plot_f1(train_F1_history, val_F1_history, plot_graphs, save_graphs)\n",
    "    \n",
    "    return x_classifier, train_loss_history, val_loss_history, train_acc_history, val_acc_history, train_F1_history, val_F1_history\n",
    "\n",
    "def test(x_classifier, test_dataset, save_plot_fname):\n",
    "    \n",
    "    def _plot_confusion_matrix(cm, classes,\n",
    "                              normalize=False,\n",
    "                              title='Confusion matrix',\n",
    "                              cmap=plt.cm.Blues,\n",
    "                              show_graphs=False, save_graphs=True):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(cm)\n",
    "\n",
    "        plt.figure(figsize=(9,6))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        if save_graphs: \n",
    "            plt.savefig(\"{}/{}_{}.png\".format(plot_path, save_plot_fname, \"conf_norm\" if normalize else \"conf\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "        if show_graphs: \n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    # Performance metrics\n",
    "    test_loss_avg = tfe.metrics.Mean() # loss\n",
    "    test_acc = tfe.metrics.Accuracy() # accuracy\n",
    "    test_f1 = tfe.metrics.Mean() # f1\n",
    "    predictions = tf.convert_to_tensor([], dtype=tf.int64)\n",
    "    correct = tf.convert_to_tensor([], dtype=tf.int64)\n",
    "    \n",
    "    # Run testing loop batch-wise\n",
    "    with tf.device(device):\n",
    "        for batch, (tst_img, tst_lbl) in enumerate(test_dataset):\n",
    "            tst_loss = calculate_loss(x_classifier, tst_img, tst_lbl)\n",
    "            test_loss_avg(tst_loss)\n",
    "            test_acc(tf.argmax(x_classifier(tst_img), axis=1), tst_lbl)\n",
    "            test_f1(calculate_f1(x_classifier, tst_img, tst_lbl))\n",
    "            predictions = tf.concat([predictions, tf.argmax(x_classifier(tst_img), axis=1)], 0)\n",
    "            correct = tf.concat([correct, tst_lbl], 0)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(correct, predictions)\n",
    "    class_names = ['normal', 'bacterial', 'viral']\n",
    "    # Plot non-normalized confusion matrix\n",
    "    _plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "    # Plot normalized confusion matrix\n",
    "    _plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,show_graphs=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "    \n",
    "    print(\"\\nTest dataset metrics: F1: {:.3f}, Loss: {:.3f}, Accuracy: {:.3%}\".format(\n",
    "        test_f1.result(), test_loss_avg.result(), test_acc.result()))\n",
    "    return test_f1.result(), test_loss_avg.result(), test_acc.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bottle neck layers and create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, dataset_type='train'):\n",
    "    \n",
    "    data = np.load(path.format(dataset_type))\n",
    "    data_bottle_necks, data_labels, data_file_paths = data['bottle_necks'],  data['labels'], data['paths']\n",
    "    \n",
    "    return data_bottle_necks, data_labels, data_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bottle_necks, train_labels, train_file_paths = load_data(save_path, 'train')\n",
    "validate_bottle_necks, validate_labels, validate_file_paths = load_data(save_path, 'validate')\n",
    "test_bottle_necks, test_labels, test_file_paths = load_data(save_path, 'test')\n",
    "n = len(train_bottle_necks)\n",
    "\n",
    "train_images_dataset = tf.data.Dataset.from_tensor_slices(train_bottle_necks)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "train_dataset_original = tf.data.Dataset.zip((train_images_dataset, train_labels_dataset))\n",
    "validate_images_dataset = tf.data.Dataset.from_tensor_slices(validate_bottle_necks)\n",
    "validate_labels_dataset = tf.data.Dataset.from_tensor_slices(validate_labels)\n",
    "validate_dataset_original = tf.data.Dataset.zip((validate_images_dataset, validate_labels_dataset))\n",
    "test_images_dataset = tf.data.Dataset.from_tensor_slices(test_bottle_necks)\n",
    "test_labels_dataset = tf.data.Dataset.from_tensor_slices(test_labels)\n",
    "test_dataset_original = tf.data.Dataset.zip((test_images_dataset, test_labels_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model # | # of hidden layers | # of nodes                            | batch size | epochs | learning rate | train_loss | test_loss | train_acc | test_acc | train_F1 | test_F1 |\n",
    "|-:-:-----|-:-:----------------|-:-:-----------------------------------|-:-:--------|-:-:----|-:-:-----------|-:-:--------|-:-:-------|-:-:-------|-:-:------|-:-:------|-:-:-----|\n",
    "| 1       | 3                  | 1024, 512, 3                          | 64         | 50     | 0.01          | 0.599      | 0.832     | 68.854%   | 56.757%  | 0.652    | 0.665   |\n",
    "| 2       | 3                  | 1024, 512, 3                          | 32         | 50     | 0.01          | 1.100      | 1.099     | 32.960%   | 33.333%  | nan      | 0.666   |\n",
    "| 3       | 3                  | 1024, 512, 3                          | 64         | 50     | 0.1           | 1.103      | 1.106     | 33.706%   | 33.333%  | nan      | 0.658   |\n",
    "| 4       | 3                  | 1024, 512, 3                          | 64         | 50     | 0.001         | 0.418      | 1.197     | 81.854%   | 68.018%  | 0.665    | 0.587   |\n",
    "| 5       | 3                  | 1024, 512, 3                          | 64         | 50     | 0.0001        | 0.269      | 1.216     | 90.455%   | 69.820%  | 0.665    | 0.655   |\n",
    "| 6       | 3                  | 1024, 512, 3                          | 64         | 100    | 0.01          | 0.560      | 1.310     | 70.296%   | 49.550%  | 0.704    | 0.681   |\n",
    "| 7       | 3                  | 1024, 512, 3                          | 64         | 200    | 0.01          | 1.099      | 1.099     | 32.911%   | 33.333%  | nan      | 0.666   |\n",
    "| 8       | 3                  | 1024, 512, 3                          | 1000       | 200    | 0.01          | 1.100      | 1.099     | 33.333%   | 33.333%  | 0.669    | 0.667   |\n",
    "| 9       | 3                  | 1024, 512, 3                          | 1000       | 200    | 0.001         | 0.413      | 0.978     | 80.661%   | 70.721%  | 0.643    | 0.627   |\n",
    "| 10      | 3                  | 1024, 512, 3                          | 1000       | 400    | 0.01          | 0.432      | 1.144     | 79.021%   | 65.315%  | 0.689    | 0.577   |\n",
    "| 11      | 3                  | 1024, 512, 3                          | 1000       | 400    | 0.001         | 0.193      | 1.232     | 91.897%   | 83.784%  | 0.670    | 0.597   |\n",
    "| 12      | 6                  | 1024, 512, 256, 128, 64, 3            | 64         | 50     | 0.01          | 0.522      | 1.142     | 67.810%   | 54.505%  | 0.716    | 0.703   |\n",
    "| 13      | 6                  | 1024, 512, 256, 128, 64, 3            | 32         | 50     | 0.01          | 0.535      | 1.107     | 73.080%   | 61.712%  | 0.675    | 0.614   |\n",
    "| 14      | 6                  | 1024, 512, 256, 128, 64, 3            | 64         | 50     | 0.1           | 1.102      | 1.103     | 33.656%   | 33.333%  | nan      | 0.665   |\n",
    "| 15      | 6                  | 1024, 512, 256, 128, 64, 3            | 64         | 50     | 0.001         | 0.407      | 0.971     | 83.246%   | 72.523%  | 0.665    | 0.636   |\n",
    "| 16      | 6                  | 1024, 512, 256, 128, 64, 3            | 64         | 50     | 0.0001        | 0.276      | 1.079     | 90.157%   | 74.775%  | 0.668    | 0.624   |\n",
    "| 17      | 6                  | 1024, 512, 256, 128, 64, 3            | 64         | 100    | 0.01          | 0.414      | 1.008     | 81.979%   | 77.027%  | 0.662    | 0.619   |\n",
    "| 18      | 6                  | 1024, 512, 256, 128, 64, 3            | 64         | 200    | 0.01          | 0.373      | 1.269     | 83.669%   | 68.468%  | 0.670    | 0.598   |\n",
    "| 19      | 6                  | 1024, 512, 256, 128, 64, 3            | 1000       | 200    | 0.01          | 0.469      | 1.045     | 79.443%   | 68.468%  | 0.646    | 0.612   |\n",
    "| 20      | 6                  | 1024, 512, 256, 128, 64, 3            | 1000       | 200    | 0.001         | 0.369      | 1.088     | 82.351%   | 68.468%  | 0.661    | 0.598   |\n",
    "| 21      | 6                  | 1024, 512, 256, 128, 64, 3            | 1000       | 400    | 0.01          | 0.333      | 1.004     | 84.439%   | 67.117%  | 0.690    | 0.641   |\n",
    "| 22      | 6                  | 1024, 512, 256, 128, 64, 3            | 1000       | 400    | 0.001         | 0.411      | 1.493     | 80.686%   | 56.306%  | 0.648    | 0.628   |\n",
    "| 23      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 64         | 50     | 0.01          | 1.099      | 1.099     | 32.563%   | 33.333%  | nan      | 0.665   |\n",
    "| 24      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 32         | 50     | 0.01          | 1.099      | 1.099     | 33.209%   | 33.333%  | nan      | 0.499   |\n",
    "| 25      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 64         | 50     | 0.1           | 1.103      | 1.107     | 33.905%   | 33.333%  | nan      | 0.494   |\n",
    "| 26      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 64         | 50     | 0.001         | 1.099      | 1.099     | 32.314%   | 33.333%  | nan      | 0.654   |\n",
    "| 27      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 64         | 50     | 0.0001        | 0.376      | 0.907     | 84.638%   | 74.324%  | 0.662    | 0.601   |\n",
    "| 28      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 64         | 100    | 0.01          | 1.099      | 1.099     | 32.438%   | 33.333%  | nan      | 0.663   |\n",
    "| 29      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 64         | 200    | 0.01          | 1.099      | 1.099     | 32.637%   | 33.333%  | nan      | 0.670   |\n",
    "| 30      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 1000       | 200    | 0.01          | 0.574      | 1.344     | 66.841%   | 40.090%  | 0.745    | 0.687   |\n",
    "| 31      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 1000       | 200    | 0.001         | 1.099      | 1.099     | 33.333%   | 33.333%  | 0.664    | 0.667   |\n",
    "| 32      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 1000       | 400    | 0.01          | 0.466      | 0.792     | 78.673%   | 76.577%  | 0.681    | 0.612   |\n",
    "| 33      | 9                  | 1024, 512, 256, 128, 64, 32, 16, 8, 3 | 1000       | 400    | 0.001         | 1.098      | 1.099     | 33.333%   | 33.333%  | 0.674    | 0.667   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [None, \"1024, 512, 3\", \"1024, 512, 256, 128, 64, 3\", \"1024, 512, 256, 128, 64, 32, 16, 8, 3\"]\n",
    "# Parameters to try out\n",
    "list_n_layers = [3] * 11 + [6] * 11 + [9] * 11\n",
    "list_batch_size = ([64, 32] + [64] * 5 + [1000] * 4) * 3\n",
    "list_n_epochs = [50, 50, 50, 50, 50, 100, 200, 200, 200, 400, 400] * 3\n",
    "list_learning_rate = [0.01, 0.01, 0.1, 0.001, 0.0001, 0.01, 0.01, 0.01, 0.001, 0.01, 0.001] * 3\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| Model number 01: L3.BS64.LR0.01.EP50 |\n",
      "----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:120.052, Train Acc:40.418%, Val F1:65.376%, Val Loss: 5.430, Val Acc: 29.279%\n",
      "Epoch: 49, Batch: 60, Loss: 0.64020133018493654\n",
      "Test dataset metrics: F1: 0.665, Loss: 0.832, Accuracy: 56.757%\n",
      "----------------------------------------\n",
      "| Model number 02: L3.BS32.LR0.01.EP50 |\n",
      "----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:51.985, Train Acc:33.731%, Val F1:49.823%, Val Loss: 1.101, Val Acc: 33.333%\n",
      "Epoch: 49, Batch: 120, Loss: 1.0965318679809572\n",
      "Test dataset metrics: F1: 0.666, Loss: 1.099, Accuracy: 33.333%\n",
      "---------------------------------------\n",
      "| Model number 03: L3.BS64.LR0.1.EP50 |\n",
      "---------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:13887.011, Train Acc:38.255%, Val F1:68.914%, Val Loss: 195.944, Val Acc: 48.649%\n",
      "Epoch: 49, Batch: 60, Loss: 1.0912874937057495\n",
      "Test dataset metrics: F1: 0.658, Loss: 1.106, Accuracy: 33.333%\n",
      "-----------------------------------------\n",
      "| Model number 04: L3.BS64.LR0.001.EP50 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:12.824, Train Acc:52.921%, Val F1:72.932%, Val Loss: 1.316, Val Acc: 55.405%\n",
      "Epoch: 49, Batch: 60, Loss: 0.45069235563278216\n",
      "Test dataset metrics: F1: 0.587, Loss: 1.197, Accuracy: 68.018%\n",
      "------------------------------------------\n",
      "| Model number 05: L3.BS64.LR0.0001.EP50 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:2.420, Train Acc:60.701%, Val F1:69.539%, Val Loss: 1.789, Val Acc: 45.045%\n",
      "Epoch: 49, Batch: 60, Loss: 0.36419725418090827\n",
      "Test dataset metrics: F1: 0.655, Loss: 1.216, Accuracy: 69.820%\n",
      "-----------------------------------------\n",
      "| Model number 06: L3.BS64.LR0.01.EP100 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:122.783, Train Acc:45.986%, Val F1:18.926%, Val Loss: 2.990, Val Acc: 35.586%\n",
      "Epoch: 99, Batch: 60, Loss: 0.54537969827651983\n",
      "Test dataset metrics: F1: 0.681, Loss: 1.310, Accuracy: 49.550%\n",
      "-----------------------------------------\n",
      "| Model number 07: L3.BS64.LR0.01.EP200 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:113.819, Train Acc:39.722%, Val F1:66.354%, Val Loss: 1.101, Val Acc: 34.234%\n",
      "Epoch 100: Train F1:0.689, Train Loss:0.625, Train Acc:67.860%, Val F1:66.824%, Val Loss: 0.755, Val Acc: 68.018%\n",
      "Epoch: 199, Batch: 60, Loss: 1.09908843040466355\n",
      "Test dataset metrics: F1: 0.666, Loss: 1.099, Accuracy: 33.333%\n",
      "-------------------------------------------\n",
      "| Model number 08: L3.BS1000.LR0.01.EP200 |\n",
      "-------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:1048.545, Train Acc:35.570%, Val F1:66.667%, Val Loss: 139.801, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.681, Train Loss:1.097, Train Acc:33.333%, Val F1:66.667%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 199, Batch: 0, Loss: 1.1004414558410645\n",
      "Test dataset metrics: F1: 0.667, Loss: 1.099, Accuracy: 33.333%\n",
      "--------------------------------------------\n",
      "| Model number 09: L3.BS1000.LR0.001.EP200 |\n",
      "--------------------------------------------\n",
      "Epoch 000: Train F1:0.571, Train Loss:107.931, Train Acc:32.985%, Val F1:66.667%, Val Loss: 21.999, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.654, Train Loss:0.487, Train Acc:76.684%, Val F1:60.166%, Val Loss: 0.634, Val Acc: 74.775%\n",
      "Epoch: 199, Batch: 0, Loss: 0.60106170177459726\n",
      "Test dataset metrics: F1: 0.627, Loss: 0.978, Accuracy: 70.721%\n",
      "-------------------------------------------\n",
      "| Model number 10: L3.BS1000.LR0.01.EP400 |\n",
      "-------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:1020.039, Train Acc:34.725%, Val F1:50.000%, Val Loss: 136.544, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.673, Train Loss:0.606, Train Acc:73.353%, Val F1:59.557%, Val Loss: 1.224, Val Acc: 63.514%\n",
      "Epoch 200: Train F1:0.665, Train Loss:0.433, Train Acc:79.443%, Val F1:61.635%, Val Loss: 0.975, Val Acc: 71.622%\n",
      "Epoch 300: Train F1:0.647, Train Loss:0.524, Train Acc:77.529%, Val F1:63.793%, Val Loss: 1.143, Val Acc: 60.360%\n",
      "Epoch: 399, Batch: 0, Loss: 0.50281983613967936\n",
      "Test dataset metrics: F1: 0.577, Loss: 1.144, Accuracy: 65.315%\n",
      "--------------------------------------------\n",
      "| Model number 11: L3.BS1000.LR0.001.EP400 |\n",
      "--------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:90.936, Train Acc:40.542%, Val F1:nan%, Val Loss: 34.974, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.662, Train Loss:0.436, Train Acc:79.841%, Val F1:59.036%, Val Loss: 0.931, Val Acc: 70.270%\n",
      "Epoch 200: Train F1:0.662, Train Loss:0.309, Train Acc:84.315%, Val F1:63.913%, Val Loss: 0.915, Val Acc: 76.577%\n",
      "Epoch 300: Train F1:0.678, Train Loss:0.260, Train Acc:88.193%, Val F1:57.926%, Val Loss: 2.264, Val Acc: 59.009%\n",
      "Epoch: 399, Batch: 0, Loss: 0.22201259434223175\n",
      "Test dataset metrics: F1: 0.597, Loss: 1.232, Accuracy: 68.468%\n",
      "----------------------------------------\n",
      "| Model number 12: L6.BS64.LR0.01.EP50 |\n",
      "----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:69.690, Train Acc:48.322%, Val F1:71.048%, Val Loss: 0.844, Val Acc: 54.505%\n",
      "Epoch: 49, Batch: 60, Loss: 0.50342369079589846\n",
      "Test dataset metrics: F1: 0.703, Loss: 1.142, Accuracy: 54.505%\n",
      "----------------------------------------\n",
      "| Model number 13: L6.BS32.LR0.01.EP50 |\n",
      "----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:19.268, Train Acc:54.735%, Val F1:60.235%, Val Loss: 0.760, Val Acc: 64.865%\n",
      "Epoch: 49, Batch: 120, Loss: 0.61605697870254526\n",
      "Test dataset metrics: F1: 0.614, Loss: 1.107, Accuracy: 61.712%\n",
      "---------------------------------------\n",
      "| Model number 14: L6.BS64.LR0.1.EP50 |\n",
      "---------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:722522.695, Train Acc:33.607%, Val F1:49.340%, Val Loss: 1.103, Val Acc: 33.333%\n",
      "Epoch: 49, Batch: 60, Loss: 1.0945296287536628\n",
      "Test dataset metrics: F1: 0.665, Loss: 1.103, Accuracy: 33.333%\n",
      "-----------------------------------------\n",
      "| Model number 15: L6.BS64.LR0.001.EP50 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:3.560, Train Acc:49.192%, Val F1:55.094%, Val Loss: 0.844, Val Acc: 59.459%\n",
      "Epoch: 49, Batch: 60, Loss: 0.62411493062973026\n",
      "Test dataset metrics: F1: 0.612, Loss: 0.971, Accuracy: 72.523%\n",
      "------------------------------------------\n",
      "| Model number 16: L6.BS64.LR0.0001.EP50 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:0.625, Train Loss:1.317, Train Acc:62.714%, Val F1:62.927%, Val Loss: 0.774, Val Acc: 71.171%\n",
      "Epoch: 49, Batch: 60, Loss: 0.38248047232627875\n",
      "Test dataset metrics: F1: 0.624, Loss: 1.079, Accuracy: 74.775%\n",
      "-----------------------------------------\n",
      "| Model number 17: L6.BS64.LR0.01.EP100 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:68.390, Train Acc:52.225%, Val F1:65.278%, Val Loss: 0.780, Val Acc: 63.514%\n",
      "Epoch: 99, Batch: 60, Loss: 0.44318139553070075\n",
      "Test dataset metrics: F1: 0.619, Loss: 1.008, Accuracy: 77.027%\n",
      "-----------------------------------------\n",
      "| Model number 18: L6.BS64.LR0.01.EP200 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:39.116, Train Acc:48.919%, Val F1:70.591%, Val Loss: 0.886, Val Acc: 52.252%\n",
      "Epoch 100: Train F1:0.667, Train Loss:0.418, Train Acc:81.233%, Val F1:58.677%, Val Loss: 1.245, Val Acc: 68.919%\n",
      "Epoch: 199, Batch: 60, Loss: 0.41421580314636233\n",
      "Test dataset metrics: F1: 0.598, Loss: 1.269, Accuracy: 68.468%\n",
      "-------------------------------------------\n",
      "| Model number 19: L6.BS1000.LR0.01.EP200 |\n",
      "-------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:638.875, Train Acc:33.557%, Val F1:42.236%, Val Loss: 5.727, Val Acc: 45.946%\n",
      "Epoch 100: Train F1:0.651, Train Loss:0.568, Train Acc:74.571%, Val F1:61.635%, Val Loss: 0.937, Val Acc: 72.523%\n",
      "Epoch: 199, Batch: 0, Loss: 0.51167583465576175\n",
      "Test dataset metrics: F1: 0.612, Loss: 1.045, Accuracy: 68.468%\n",
      "--------------------------------------------\n",
      "| Model number 20: L6.BS1000.LR0.001.EP200 |\n",
      "--------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:22.957, Train Acc:32.936%, Val F1:nan%, Val Loss: 19.287, Val Acc: 33.333%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train F1:0.664, Train Loss:0.436, Train Acc:78.026%, Val F1:59.394%, Val Loss: 0.898, Val Acc: 71.622%\n",
      "Epoch: 199, Batch: 0, Loss: 0.41869053244590764\n",
      "Test dataset metrics: F1: 0.598, Loss: 1.088, Accuracy: 68.468%\n",
      "-------------------------------------------\n",
      "| Model number 21: L6.BS1000.LR0.01.EP400 |\n",
      "-------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:1308.998, Train Acc:32.438%, Val F1:66.667%, Val Loss: 10.573, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.637, Train Loss:0.606, Train Acc:74.024%, Val F1:54.312%, Val Loss: 1.170, Val Acc: 52.703%\n",
      "Epoch 200: Train F1:0.663, Train Loss:0.400, Train Acc:80.413%, Val F1:60.123%, Val Loss: 0.919, Val Acc: 73.423%\n",
      "Epoch 300: Train F1:0.649, Train Loss:0.385, Train Acc:81.009%, Val F1:62.288%, Val Loss: 0.643, Val Acc: 81.081%\n",
      "Epoch: 399, Batch: 0, Loss: 0.39493727684020996\n",
      "Test dataset metrics: F1: 0.641, Loss: 1.004, Accuracy: 67.117%\n",
      "--------------------------------------------\n",
      "| Model number 22: L6.BS1000.LR0.001.EP400 |\n",
      "--------------------------------------------\n",
      "Epoch 000: Train F1:0.484, Train Loss:13.914, Train Acc:36.614%, Val F1:66.667%, Val Loss: 11.946, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.672, Train Loss:0.482, Train Acc:78.325%, Val F1:65.297%, Val Loss: 0.553, Val Acc: 81.081%\n",
      "Epoch 200: Train F1:0.700, Train Loss:0.369, Train Acc:81.904%, Val F1:62.687%, Val Loss: 0.883, Val Acc: 74.324%\n",
      "Epoch 300: Train F1:0.685, Train Loss:0.278, Train Acc:87.223%, Val F1:63.226%, Val Loss: 0.787, Val Acc: 80.631%\n",
      "Epoch: 399, Batch: 0, Loss: 0.59440326690673835\n",
      "Test dataset metrics: F1: 0.628, Loss: 1.493, Accuracy: 56.306%\n",
      "----------------------------------------\n",
      "| Model number 23: L9.BS64.LR0.01.EP50 |\n",
      "----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:8.654, Train Acc:32.339%, Val F1:nan%, Val Loss: 1.101, Val Acc: 33.333%\n",
      "Epoch: 49, Batch: 60, Loss: 1.0980179309844975\n",
      "Test dataset metrics: F1: 0.665, Loss: 1.099, Accuracy: 33.333%\n",
      "----------------------------------------\n",
      "| Model number 24: L9.BS32.LR0.01.EP50 |\n",
      "----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:14.973, Train Acc:33.656%, Val F1:49.888%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 49, Batch: 120, Loss: 1.0976526737213135\n",
      "Test dataset metrics: F1: 0.499, Loss: 1.099, Accuracy: 33.333%\n",
      "---------------------------------------\n",
      "| Model number 25: L9.BS64.LR0.1.EP50 |\n",
      "---------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:190381.183, Train Acc:32.414%, Val F1:66.543%, Val Loss: 1.100, Val Acc: 33.333%\n",
      "Epoch: 49, Batch: 60, Loss: 1.0949468612670898\n",
      "Test dataset metrics: F1: 0.494, Loss: 1.107, Accuracy: 33.333%\n",
      "-----------------------------------------\n",
      "| Model number 26: L9.BS64.LR0.001.EP50 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:1.712, Train Acc:36.987%, Val F1:66.833%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 49, Batch: 60, Loss: 1.0987163782119755\n",
      "Test dataset metrics: F1: 0.654, Loss: 1.099, Accuracy: 33.333%\n",
      "------------------------------------------\n",
      "| Model number 27: L9.BS64.LR0.0001.EP50 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:0.703, Train Loss:0.824, Train Acc:60.974%, Val F1:69.082%, Val Loss: 0.844, Val Acc: 58.559%\n",
      "Epoch: 49, Batch: 60, Loss: 0.44356936216354374\n",
      "Test dataset metrics: F1: 0.601, Loss: 0.907, Accuracy: 74.324%\n",
      "-----------------------------------------\n",
      "| Model number 28: L9.BS64.LR0.01.EP100 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:8.599, Train Acc:34.004%, Val F1:66.898%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 99, Batch: 60, Loss: 1.0970234870910645\n",
      "Test dataset metrics: F1: 0.663, Loss: 1.099, Accuracy: 33.333%\n",
      "-----------------------------------------\n",
      "| Model number 29: L9.BS64.LR0.01.EP200 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:15.864, Train Acc:33.209%, Val F1:66.171%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:nan, Train Loss:1.099, Train Acc:32.737%, Val F1:66.111%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 199, Batch: 60, Loss: 1.0985050201416016\n",
      "Test dataset metrics: F1: 0.670, Loss: 1.099, Accuracy: 33.333%\n",
      "-------------------------------------------\n",
      "| Model number 30: L9.BS1000.LR0.01.EP200 |\n",
      "-------------------------------------------\n",
      "Epoch 000: Train F1:0.634, Train Loss:330.921, Train Acc:33.159%, Val F1:66.667%, Val Loss: 4.456, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.570, Train Loss:0.573, Train Acc:63.236%, Val F1:52.763%, Val Loss: 1.001, Val Acc: 47.297%\n",
      "Epoch: 199, Batch: 0, Loss: 0.6096513867378235\n",
      "Test dataset metrics: F1: 0.687, Loss: 1.344, Accuracy: 40.090%\n",
      "--------------------------------------------\n",
      "| Model number 31: L9.BS1000.LR0.001.EP200 |\n",
      "--------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:1.110, Train Acc:33.333%, Val F1:50.000%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.672, Train Loss:1.098, Train Acc:33.333%, Val F1:66.667%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 199, Batch: 0, Loss: 1.0997748374938965\n",
      "Test dataset metrics: F1: 0.667, Loss: 1.099, Accuracy: 33.333%\n",
      "-------------------------------------------\n",
      "| Model number 32: L9.BS1000.LR0.01.EP400 |\n",
      "-------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:104.839, Train Acc:32.612%, Val F1:50.000%, Val Loss: 1.561, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.675, Train Loss:0.471, Train Acc:77.604%, Val F1:59.319%, Val Loss: 1.274, Val Acc: 67.117%\n",
      "Epoch 200: Train F1:0.666, Train Loss:0.446, Train Acc:78.275%, Val F1:59.919%, Val Loss: 1.196, Val Acc: 68.468%\n",
      "Epoch 300: Train F1:0.660, Train Loss:0.464, Train Acc:78.797%, Val F1:63.656%, Val Loss: 0.841, Val Acc: 75.225%\n",
      "Epoch: 399, Batch: 0, Loss: 0.50312048196792625\n",
      "Test dataset metrics: F1: 0.612, Loss: 0.792, Accuracy: 76.577%\n",
      "--------------------------------------------\n",
      "| Model number 33: L9.BS1000.LR0.001.EP400 |\n",
      "--------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:1.542, Train Acc:33.880%, Val F1:50.000%, Val Loss: 1.645, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.688, Train Loss:1.097, Train Acc:33.333%, Val F1:66.667%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 200: Train F1:0.658, Train Loss:1.100, Train Acc:33.333%, Val F1:66.667%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 300: Train F1:0.641, Train Loss:1.101, Train Acc:33.333%, Val F1:66.667%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 399, Batch: 0, Loss: 1.0996789932250977\n",
      "Test dataset metrics: F1: 0.667, Loss: 1.099, Accuracy: 33.333%\n",
      "\n",
      "| Model # | # of hidden layers |               # of nodes              | batch size | epochs | learning rate | train_loss | val_loss | test_loss | train_acc | val_acc | test_acc | train_F1 | val_F1 | test_F1 | \n",
      "|:-------:|:------------------:|:-------------------------------------:|:----------:|:------:|:-------------:|:----------:|:--------:|:---------:|:---------:|:-------:|:--------:|:--------:|:------:|:-------:|\n",
      "\n",
      "|2|3|1024, 512, 3|64|50|0.01|0.599|0.832|0.832|68.854%|56.757%|56.757%|0.652|0.665|0.665|\n",
      "|3|3|1024, 512, 3|32|50|0.01|1.100|1.099|1.099|32.960%|33.333%|33.333%|nan|0.665|0.666|\n",
      "|4|3|1024, 512, 3|64|50|0.1|1.103|1.102|1.106|33.706%|33.333%|33.333%|nan|0.665|0.658|\n",
      "|5|3|1024, 512, 3|64|50|0.001|0.418|1.171|1.197|81.854%|68.018%|68.018%|0.665|0.595|0.587|\n",
      "|6|3|1024, 512, 3|64|50|0.0001|0.269|1.140|1.216|90.455%|69.820%|69.820%|0.665|0.654|0.655|\n",
      "|7|3|1024, 512, 3|64|100|0.01|0.560|1.241|1.310|70.296%|49.550%|49.550%|0.704|0.679|0.681|\n",
      "|8|3|1024, 512, 3|64|200|0.01|1.099|1.099|1.099|32.911%|33.333%|33.333%|nan|0.660|0.666|\n",
      "|9|3|1024, 512, 3|1000|200|0.01|1.100|1.099|1.099|33.333%|33.333%|33.333%|0.669|0.667|0.667|\n",
      "|10|3|1024, 512, 3|1000|200|0.001|0.413|0.978|0.978|80.661%|70.721%|70.721%|0.643|0.627|0.627|\n",
      "|11|3|1024, 512, 3|1000|400|0.01|0.432|1.144|1.144|79.021%|65.315%|65.315%|0.689|0.577|0.577|\n",
      "|12|3|1024, 512, 3|1000|400|0.001|0.193|1.232|1.232|91.897%|68.468%|68.468%|0.670|0.597|0.597|\n",
      "|13|6|1024, 512, 256, 128, 64, 3|64|50|0.01|0.522|1.116|1.142|67.810%|54.505%|54.505%|0.716|0.706|0.703|\n",
      "|14|6|1024, 512, 256, 128, 64, 3|32|50|0.01|0.535|1.108|1.107|73.080%|61.712%|61.712%|0.675|0.614|0.614|\n",
      "|15|6|1024, 512, 256, 128, 64, 3|64|50|0.1|1.102|1.103|1.103|33.656%|33.333%|33.333%|nan|0.669|0.665|\n",
      "|16|6|1024, 512, 256, 128, 64, 3|64|50|0.001|0.407|0.985|0.971|83.246%|72.523%|72.523%|0.665|0.612|0.612|\n",
      "|17|6|1024, 512, 256, 128, 64, 3|64|50|0.0001|0.276|1.077|1.079|90.157%|74.775%|74.775%|0.668|0.605|0.624|\n",
      "|18|6|1024, 512, 256, 128, 64, 3|64|100|0.01|0.414|0.960|1.008|81.979%|77.027%|77.027%|0.662|0.620|0.619|\n",
      "|19|6|1024, 512, 256, 128, 64, 3|64|200|0.01|0.373|1.216|1.269|83.669%|68.468%|68.468%|0.670|0.599|0.598|\n",
      "|20|6|1024, 512, 256, 128, 64, 3|1000|200|0.01|0.469|1.045|1.045|79.443%|68.468%|68.468%|0.646|0.612|0.612|\n",
      "|21|6|1024, 512, 256, 128, 64, 3|1000|200|0.001|0.369|1.088|1.088|82.351%|68.468%|68.468%|0.661|0.598|0.598|\n",
      "|22|6|1024, 512, 256, 128, 64, 3|1000|400|0.01|0.333|1.004|1.004|84.439%|67.117%|67.117%|0.690|0.641|0.641|\n",
      "|23|6|1024, 512, 256, 128, 64, 3|1000|400|0.001|0.411|1.493|1.493|80.686%|56.306%|56.306%|0.648|0.628|0.628|\n",
      "|24|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|64|50|0.01|1.099|1.099|1.099|32.563%|33.333%|33.333%|nan|0.661|0.665|\n",
      "|25|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|32|50|0.01|1.099|1.099|1.099|33.209%|33.333%|33.333%|nan|0.497|0.499|\n",
      "|26|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|64|50|0.1|1.103|1.102|1.107|33.905%|33.333%|33.333%|nan|0.499|0.494|\n",
      "|27|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|64|50|0.001|1.099|1.099|1.099|32.314%|33.333%|33.333%|nan|0.671|0.654|\n",
      "|28|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|64|50|0.0001|0.376|0.882|0.907|84.638%|74.324%|74.324%|0.662|0.603|0.601|\n",
      "|29|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|64|100|0.01|1.099|1.099|1.099|32.438%|33.333%|33.333%|nan|0.669|0.663|\n",
      "|30|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|64|200|0.01|1.099|1.099|1.099|32.637%|33.333%|33.333%|nan|0.656|0.670|\n",
      "|31|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|1000|200|0.01|0.574|1.344|1.344|66.841%|40.090%|40.090%|0.745|0.687|0.687|\n",
      "|32|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|1000|200|0.001|1.099|1.099|1.099|33.333%|33.333%|33.333%|0.664|0.667|0.667|\n",
      "|33|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|1000|400|0.01|0.466|0.792|0.792|78.673%|76.577%|76.577%|0.681|0.612|0.612|\n",
      "|34|9|1024, 512, 256, 128, 64, 32, 16, 8, 3|1000|400|0.001|1.098|1.099|1.099|33.333%|33.333%|33.333%|0.674|0.667|0.667|\n"
     ]
    }
   ],
   "source": [
    "markdown = \"\"\"\n",
    "| Model # | # of hidden layers |               # of nodes              | batch size | epochs | learning rate | train_loss | val_loss | test_loss | train_acc | val_acc | test_acc | train_F1 | val_F1 | test_F1 | \n",
    "|:-------:|:------------------:|:-------------------------------------:|:----------:|:------:|:-------------:|:----------:|:--------:|:---------:|:---------:|:-------:|:--------:|:--------:|:------:|:-------:|\n",
    "\"\"\"\n",
    "for i, (n_layers, batch_size, n_epochs, learning_rate) in \\\n",
    "    enumerate(zip(list_n_layers, list_batch_size, list_n_epochs, list_learning_rate), 1):\n",
    "\n",
    "    save_plot_fname = \"L{}.BS{}.LR{}.EP{}\".format(n_layers, batch_size, learning_rate, n_epochs)\n",
    "    title = \"| Model number {:02d}: {} |\".format(i, save_plot_fname)\n",
    "    print(\"-\"*len(title))\n",
    "    print(title)\n",
    "    print(\"-\"*len(title))\n",
    "    \n",
    "    train_dataset = train_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "    validate_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "    test_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "    \n",
    "    x_classifier, tl, vl, ta, va, tf1, vf1 = train(train_dataset, validate_dataset, learning_rate, batch_size, n_epochs, n_layers, n_classes, save_plot_fname)\n",
    "    tstf1, tstl, tsta = test(x_classifier, test_dataset, save_plot_fname)\n",
    "    markdown += \"\\n|{}|{}|{}|{}|{}|{}|{:.3f}|{:.3f}|{:.3f}|{:.3%}|{:.3%}|{:.3%}|{:.3f}|{:.3f}|{:.3f}|\".format(str(i), n_layers, shapes[n_layers//3], batch_size, n_epochs, learning_rate, tl[-1].numpy(), vl[-1].numpy(), tstl.numpy(), ta[-1].numpy(), va[-1].numpy(), tsta.numpy(), tf1[-1].numpy(), vf1[-1].numpy(), tstf1.numpy())\n",
    "    \n",
    "print(markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything below has been incorporated into the main loop above\n",
    "\n",
    "\n",
    "### Further Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| Model: L6.BS1000.LR0.001.EP400 |\n",
      "----------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:18.013, Train Acc:33.582%, Val F1:66.667%, Val Loss: 15.058, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.658, Train Loss:0.585, Train Acc:74.298%, Val F1:60.206%, Val Loss: 0.753, Val Acc: 71.622%\n",
      "Epoch 200: Train F1:0.662, Train Loss:0.454, Train Acc:79.468%, Val F1:58.383%, Val Loss: 1.141, Val Acc: 66.216%\n",
      "Epoch 300: Train F1:0.648, Train Loss:0.327, Train Acc:84.017%, Val F1:56.061%, Val Loss: 1.404, Val Acc: 59.459%\n",
      "Epoch: 399, Batch: 0, Loss: 0.48889654874801636\n",
      "Test dataset metrics: F1: 0.614, Loss: 0.659, Accuracy: 77.477%\n"
     ]
    }
   ],
   "source": [
    "n_classes, n_layers, batch_size, learning_rate, n_epochs = 3, 3, 1000, 0.001, 400\n",
    "save_plot_fname = \"L{}.BS{}.LR{}.EP{}\".format(n_layers, batch_size, learning_rate, n_epochs)\n",
    "title = \"| Model: {} |\".format(save_plot_fname)\n",
    "print(\"-\"*len(title))\n",
    "print(title)\n",
    "print(\"-\"*len(title))\n",
    "\n",
    "train_dataset = train_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "validate_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "test_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "\n",
    "x_classifier, tl, vl, ta, va, tf1, vf1 = train(train_dataset, validate_dataset, learning_rate, batch_size, n_epochs, n_layers, n_classes, save_plot_fname)\n",
    "test(x_classifier, test_dataset, save_plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to try out\n",
    "list_n_layers = [3] * 4 + [6] * 4\n",
    "list_batch_size = [1000] * 8\n",
    "list_n_epochs = [200, 200, 400, 400] * 2\n",
    "list_learning_rate = [0.01, 0.001] * 4\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| Model number 01: L3.BS64.LR0.01.EP200 |\n",
      "-----------------------------------------\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 000: Train F1:nan, Train Loss:59.788, Train Acc:51.479%, Val F1:51.945%, Val Loss: 1.614, Val Acc: 54.054%\n",
      "Epoch 100: Train F1:0.708, Train Loss:0.528, Train Acc:65.722%, Val F1:72.442%, Val Loss: 1.117, Val Acc: 52.703%\n",
      "Epoch: 199, Batch: 60, Loss: 0.65268450975418094\n",
      "Test dataset metrics: F1: 0.722, Loss: 0.988, Accuracy: 53.153%\n",
      "------------------------------------------\n",
      "| Model number 02: L3.BS64.LR0.001.EP200 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:9.216, Train Acc:58.886%, Val F1:62.853%, Val Loss: 0.959, Val Acc: 67.568%\n",
      "Epoch 100: Train F1:0.666, Train Loss:0.218, Train Acc:92.468%, Val F1:62.995%, Val Loss: 1.305, Val Acc: 72.973%\n",
      "Epoch: 199, Batch: 60, Loss: 0.119583688676357276\n",
      "Test dataset metrics: F1: 0.638, Loss: 2.745, Accuracy: 63.514%\n",
      "-----------------------------------------\n",
      "| Model number 03: L3.BS64.LR0.01.EP400 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:69.583, Train Acc:55.307%, Val F1:65.298%, Val Loss: 0.929, Val Acc: 56.757%\n",
      "Epoch 100: Train F1:nan, Train Loss:1.099, Train Acc:32.985%, Val F1:50.240%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 200: Train F1:nan, Train Loss:1.099, Train Acc:32.861%, Val F1:50.281%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 300: Train F1:nan, Train Loss:1.099, Train Acc:32.985%, Val F1:50.004%, Val Loss: 1.098, Val Acc: 33.333%\n",
      "Epoch: 399, Batch: 60, Loss: 1.0989236831665041\n",
      "Test dataset metrics: F1: 0.503, Loss: 1.099, Accuracy: 33.333%\n",
      "------------------------------------------\n",
      "| Model number 04: L3.BS64.LR0.001.EP400 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:0.617, Train Loss:7.996, Train Acc:60.925%, Val F1:56.507%, Val Loss: 0.894, Val Acc: 67.117%\n",
      "Epoch 100: Train F1:0.667, Train Loss:0.208, Train Acc:93.164%, Val F1:64.014%, Val Loss: 1.665, Val Acc: 62.613%\n",
      "Epoch 200: Train F1:0.669, Train Loss:0.122, Train Acc:96.321%, Val F1:59.552%, Val Loss: 2.958, Val Acc: 68.018%\n",
      "Epoch 300: Train F1:0.668, Train Loss:0.045, Train Acc:99.354%, Val F1:61.926%, Val Loss: 3.452, Val Acc: 68.919%\n",
      "Epoch: 399, Batch: 60, Loss: 0.0091466987505555156\n",
      "Test dataset metrics: F1: 0.613, Loss: 4.271, Accuracy: 65.766%\n",
      "-----------------------------------------\n",
      "| Model number 05: L3.BS64.LR0.01.EP800 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:59.869, Train Acc:55.580%, Val F1:65.471%, Val Loss: 0.822, Val Acc: 61.261%\n",
      "Epoch 100: Train F1:0.668, Train Loss:0.378, Train Acc:84.290%, Val F1:63.067%, Val Loss: 0.852, Val Acc: 73.423%\n",
      "Epoch 200: Train F1:0.669, Train Loss:0.356, Train Acc:85.657%, Val F1:59.650%, Val Loss: 1.098, Val Acc: 71.171%\n",
      "Epoch 300: Train F1:0.680, Train Loss:0.379, Train Acc:85.111%, Val F1:62.644%, Val Loss: 1.378, Val Acc: 68.018%\n",
      "Epoch 400: Train F1:0.733, Train Loss:0.520, Train Acc:66.120%, Val F1:71.202%, Val Loss: 1.423, Val Acc: 44.595%\n",
      "Epoch 500: Train F1:0.784, Train Loss:0.544, Train Acc:65.871%, Val F1:69.580%, Val Loss: 1.491, Val Acc: 43.243%\n",
      "Epoch 600: Train F1:0.712, Train Loss:0.523, Train Acc:65.424%, Val F1:53.244%, Val Loss: 1.277, Val Acc: 50.450%\n",
      "Epoch 700: Train F1:0.792, Train Loss:0.507, Train Acc:66.890%, Val F1:73.352%, Val Loss: 1.519, Val Acc: 53.153%\n",
      "Epoch: 799, Batch: 60, Loss: 0.63026094436645514\n",
      "Test dataset metrics: F1: 0.716, Loss: 1.446, Accuracy: 50.450%\n",
      "------------------------------------------\n",
      "| Model number 06: L3.BS64.LR0.001.EP800 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:11.899, Train Acc:56.774%, Val F1:57.714%, Val Loss: 0.845, Val Acc: 66.216%\n",
      "Epoch 100: Train F1:0.666, Train Loss:0.248, Train Acc:90.927%, Val F1:60.985%, Val Loss: 1.337, Val Acc: 69.820%\n",
      "Epoch 200: Train F1:0.669, Train Loss:0.135, Train Acc:96.147%, Val F1:62.533%, Val Loss: 2.833, Val Acc: 65.315%\n",
      "Epoch 300: Train F1:0.668, Train Loss:0.086, Train Acc:98.484%, Val F1:59.275%, Val Loss: 3.127, Val Acc: 66.216%\n",
      "Epoch 400: Train F1:0.668, Train Loss:0.044, Train Acc:99.428%, Val F1:58.409%, Val Loss: 4.415, Val Acc: 66.667%\n",
      "Epoch 500: Train F1:0.668, Train Loss:0.011, Train Acc:99.901%, Val F1:61.564%, Val Loss: 5.334, Val Acc: 67.568%\n",
      "Epoch 600: Train F1:0.669, Train Loss:0.000, Train Acc:100.000%, Val F1:61.690%, Val Loss: 5.971, Val Acc: 67.117%\n",
      "Epoch 700: Train F1:0.669, Train Loss:0.000, Train Acc:100.000%, Val F1:60.810%, Val Loss: 7.100, Val Acc: 68.919%\n",
      "Epoch: 799, Batch: 60, Loss: 1.1175865211043856e-07\n",
      "Test dataset metrics: F1: 0.613, Loss: 7.724, Accuracy: 65.766%\n",
      "-----------------------------------------\n",
      "| Model number 07: L6.BS64.LR0.01.EP200 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:36.464, Train Acc:52.200%, Val F1:55.242%, Val Loss: 0.862, Val Acc: 61.712%\n",
      "Epoch 100: Train F1:0.659, Train Loss:0.486, Train Acc:80.736%, Val F1:60.588%, Val Loss: 1.034, Val Acc: 71.622%\n",
      "Epoch: 199, Batch: 60, Loss: 0.46317782998085026\n",
      "Test dataset metrics: F1: 0.613, Loss: 1.346, Accuracy: 70.270%\n",
      "------------------------------------------\n",
      "| Model number 08: L6.BS64.LR0.001.EP200 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:2.754, Train Acc:56.376%, Val F1:61.742%, Val Loss: 0.798, Val Acc: 65.766%\n",
      "Epoch 100: Train F1:0.666, Train Loss:0.249, Train Acc:91.101%, Val F1:56.608%, Val Loss: 1.659, Val Acc: 65.766%\n",
      "Epoch: 199, Batch: 60, Loss: 0.090817146003246312\n",
      "Test dataset metrics: F1: 0.606, Loss: 2.577, Accuracy: 65.766%\n",
      "-----------------------------------------\n",
      "| Model number 09: L6.BS64.LR0.01.EP400 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:34.159, Train Acc:49.565%, Val F1:51.828%, Val Loss: 0.846, Val Acc: 54.054%\n",
      "Epoch 100: Train F1:0.664, Train Loss:0.385, Train Acc:85.011%, Val F1:64.035%, Val Loss: 1.498, Val Acc: 63.964%\n",
      "Epoch 200: Train F1:0.671, Train Loss:0.455, Train Acc:80.114%, Val F1:64.490%, Val Loss: 0.973, Val Acc: 72.973%\n",
      "Epoch 300: Train F1:0.737, Train Loss:0.499, Train Acc:66.716%, Val F1:71.769%, Val Loss: 1.388, Val Acc: 48.649%\n",
      "Epoch: 399, Batch: 60, Loss: 1.10453248023986823\n",
      "Test dataset metrics: F1: 0.504, Loss: 1.098, Accuracy: 33.333%\n",
      "------------------------------------------\n",
      "| Model number 10: L6.BS64.LR0.001.EP400 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:0.578, Train Loss:3.101, Train Acc:57.122%, Val F1:65.008%, Val Loss: 0.846, Val Acc: 62.162%\n",
      "Epoch 100: Train F1:0.665, Train Loss:0.244, Train Acc:91.225%, Val F1:61.500%, Val Loss: 1.289, Val Acc: 72.523%\n",
      "Epoch 200: Train F1:0.668, Train Loss:0.075, Train Acc:98.608%, Val F1:62.615%, Val Loss: 2.719, Val Acc: 66.667%\n",
      "Epoch 300: Train F1:0.666, Train Loss:0.102, Train Acc:98.185%, Val F1:60.819%, Val Loss: 2.838, Val Acc: 66.216%\n",
      "Epoch: 399, Batch: 60, Loss: 0.16728794574737553556\n",
      "Test dataset metrics: F1: 0.598, Loss: 3.424, Accuracy: 66.667%\n",
      "-----------------------------------------\n",
      "| Model number 11: L6.BS64.LR0.01.EP800 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:33.201, Train Acc:53.965%, Val F1:54.917%, Val Loss: 0.850, Val Acc: 63.964%\n",
      "Epoch 100: Train F1:0.720, Train Loss:0.529, Train Acc:65.797%, Val F1:71.931%, Val Loss: 1.251, Val Acc: 47.748%\n",
      "Epoch 200: Train F1:nan, Train Loss:1.098, Train Acc:33.085%, Val F1:50.031%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 300: Train F1:nan, Train Loss:1.097, Train Acc:32.936%, Val F1:50.124%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 400: Train F1:nan, Train Loss:1.097, Train Acc:33.060%, Val F1:49.886%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 500: Train F1:nan, Train Loss:1.097, Train Acc:32.737%, Val F1:49.796%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 600: Train F1:nan, Train Loss:1.097, Train Acc:32.911%, Val F1:50.359%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 700: Train F1:nan, Train Loss:1.097, Train Acc:33.134%, Val F1:49.623%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 799, Batch: 60, Loss: 1.1000009775161743\n",
      "Test dataset metrics: F1: 0.501, Loss: 1.099, Accuracy: 33.333%\n",
      "------------------------------------------\n",
      "| Model number 12: L6.BS64.LR0.001.EP800 |\n",
      "------------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:3.405, Train Acc:53.816%, Val F1:64.214%, Val Loss: 0.776, Val Acc: 67.568%\n",
      "Epoch 100: Train F1:0.666, Train Loss:0.225, Train Acc:92.294%, Val F1:61.813%, Val Loss: 1.025, Val Acc: 75.676%\n",
      "Epoch 200: Train F1:0.668, Train Loss:0.069, Train Acc:98.608%, Val F1:59.475%, Val Loss: 3.214, Val Acc: 63.964%\n",
      "Epoch 300: Train F1:0.668, Train Loss:0.106, Train Acc:98.260%, Val F1:61.887%, Val Loss: 2.429, Val Acc: 65.315%\n",
      "Epoch 400: Train F1:0.668, Train Loss:0.000, Train Acc:100.000%, Val F1:60.739%, Val Loss: 7.629, Val Acc: 67.568%\n",
      "Epoch 500: Train F1:0.668, Train Loss:0.000, Train Acc:100.000%, Val F1:60.096%, Val Loss: 9.324, Val Acc: 67.568%\n",
      "Epoch 600: Train F1:0.669, Train Loss:0.000, Train Acc:100.000%, Val F1:60.590%, Val Loss: 10.867, Val Acc: 67.568%\n",
      "Epoch 700: Train F1:0.668, Train Loss:0.049, Train Acc:99.279%, Val F1:61.901%, Val Loss: 3.400, Val Acc: 68.018%\n",
      "Epoch: 799, Batch: 60, Loss: 2.194080934714293e-067\n",
      "Test dataset metrics: F1: 0.598, Loss: 9.497, Accuracy: 65.766%\n"
     ]
    }
   ],
   "source": [
    "for i, (n_layers, batch_size, n_epochs, learning_rate) in \\\n",
    "    enumerate(zip(list_n_layers, list_batch_size, list_n_epochs, list_learning_rate), 1):\n",
    "\n",
    "    save_plot_fname = \"L{}.BS{}.LR{}.EP{}\".format(n_layers, batch_size, learning_rate, n_epochs)\n",
    "    title = \"| Model number {:02d}: {} |\".format(i, save_plot_fname)\n",
    "    print(\"-\"*len(title))\n",
    "    print(title)\n",
    "    print(\"-\"*len(title))\n",
    "    \n",
    "    train_dataset = train_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "    validate_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "    test_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "\n",
    "    x_classifier, tl, vl, ta, va, tf1, vf1 = train(train_dataset, validate_dataset, learning_rate, batch_size, n_epochs, n_layers, n_classes, save_plot_fname)\n",
    "    test(x_classifier, test_dataset, save_plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| Model number 01: L6.BS64.LR0.01.EP400 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:32.551, Train Acc:49.217%, Val F1:55.509%, Val Loss: 0.879, Val Acc: 62.613%\n",
      "Epoch 100: Train F1:0.671, Train Loss:0.378, Train Acc:83.296%, Val F1:61.397%, Val Loss: 0.934, Val Acc: 71.622%\n",
      "Epoch 200: Train F1:0.669, Train Loss:0.402, Train Acc:83.495%, Val F1:61.476%, Val Loss: 1.045, Val Acc: 69.820%\n",
      "Epoch 300: Train F1:0.661, Train Loss:0.404, Train Acc:82.053%, Val F1:56.947%, Val Loss: 1.225, Val Acc: 60.360%\n",
      "Epoch: 399, Batch: 60, Loss: 0.39327919483184814\n",
      "Test dataset metrics: F1: 0.609, Loss: 1.362, Accuracy: 64.865%\n",
      "-----------------------------------------\n",
      "| Model number 02: L6.BS64.LR0.01.EP800 |\n",
      "-----------------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:41.075, Train Acc:52.001%, Val F1:54.752%, Val Loss: 0.868, Val Acc: 53.604%\n",
      "Epoch 100: Train F1:nan, Train Loss:1.100, Train Acc:33.507%, Val F1:nan%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 200: Train F1:nan, Train Loss:1.099, Train Acc:32.662%, Val F1:50.442%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 300: Train F1:nan, Train Loss:1.099, Train Acc:32.960%, Val F1:50.309%, Val Loss: 1.098, Val Acc: 33.333%\n",
      "Epoch 400: Train F1:nan, Train Loss:1.099, Train Acc:32.985%, Val F1:49.965%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 500: Train F1:nan, Train Loss:1.099, Train Acc:33.284%, Val F1:49.588%, Val Loss: 1.098, Val Acc: 33.333%\n",
      "Epoch 600: Train F1:nan, Train Loss:1.099, Train Acc:32.762%, Val F1:49.892%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 700: Train F1:nan, Train Loss:1.099, Train Acc:33.060%, Val F1:50.427%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 799, Batch: 60, Loss: 1.0988098382949838\n",
      "Test dataset metrics: F1: 0.502, Loss: 1.099, Accuracy: 33.333%\n"
     ]
    }
   ],
   "source": [
    "list_n_layers = [6,6]\n",
    "list_batch_size = [64,64]\n",
    "list_learning_rate = [0.01,0.01]\n",
    "list_n_epochs = [400, 800]\n",
    "for i, (n_layers, batch_size, n_epochs, learning_rate) in \\\n",
    "    enumerate(zip(list_n_layers, list_batch_size, list_n_epochs, list_learning_rate), 1):\n",
    "\n",
    "    save_plot_fname = \"L{}.BS{}.LR{}.EP{}\".format(n_layers, batch_size, learning_rate, n_epochs)\n",
    "    title = \"| Model number {:02d}: {} |\".format(i, save_plot_fname)\n",
    "    print(\"-\"*len(title))\n",
    "    print(title)\n",
    "    print(\"-\"*len(title))\n",
    "    \n",
    "    train_dataset = train_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "    validate_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "    test_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "\n",
    "    x_classifier, tl, vl, ta, va, tf1, vf1 = train(train_dataset, validate_dataset, learning_rate, batch_size, n_epochs, n_layers, n_classes, save_plot_fname)\n",
    "    test(x_classifier, test_dataset, save_plot_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| Model: L3.BS1000.LR0.001.EP400 |\n",
      "----------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:100.960, Train Acc:34.178%, Val F1:nan%, Val Loss: 26.520, Val Acc: 33.333%\n",
      "Epoch 100: Train F1:0.656, Train Loss:0.404, Train Acc:80.015%, Val F1:61.216%, Val Loss: 0.636, Val Acc: 78.829%\n",
      "Epoch 200: Train F1:0.654, Train Loss:0.293, Train Acc:85.981%, Val F1:61.123%, Val Loss: 0.962, Val Acc: 74.324%\n",
      "Epoch 300: Train F1:0.663, Train Loss:0.196, Train Acc:91.723%, Val F1:59.798%, Val Loss: 1.089, Val Acc: 69.820%\n",
      "Epoch: 399, Batch: 0, Loss: 0.44639092683792114\n",
      "Test dataset metrics: F1: 0.636, Loss: 0.628, Accuracy: 83.784%\n"
     ]
    }
   ],
   "source": [
    "# parameters according to paper\n",
    "n_classes, n_layers, batch_size, learning_rate, n_epochs = 3, 3, 1000, 0.001, 400\n",
    "save_plot_fname = \"L{}.BS{}.LR{}.EP{}\".format(n_layers, batch_size, learning_rate, n_epochs)\n",
    "title = \"| Model: {} |\".format(save_plot_fname)\n",
    "print(\"-\"*len(title))\n",
    "print(title)\n",
    "print(\"-\"*len(title))\n",
    "\n",
    "train_dataset = train_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "validate_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "test_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "\n",
    "x_classifier, tl, vl, ta, va, tf1, vf1 = train(train_dataset, validate_dataset, learning_rate, batch_size, n_epochs, n_layers, n_classes, save_plot_fname)\n",
    "test(x_classifier, test_dataset, save_plot_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "### Model: L3.BS1000.LR0.001.EP400 \n",
    "\n",
    "Epoch 000: Train F1:nan, Train Loss:100.960, Train Acc:34.178%, Val F1:nan%, Val Loss: 26.520, Val Acc: 33.333%\n",
    "\n",
    "Epoch 100: Train F1:0.656, Train Loss:0.404, Train Acc:80.015%, Val F1:61.216%, Val Loss: 0.636, Val Acc: 78.829%\n",
    "\n",
    "Epoch 200: Train F1:0.654, Train Loss:0.293, Train Acc:85.981%, Val F1:61.123%, Val Loss: 0.962, Val Acc: 74.324%\n",
    "\n",
    "Epoch 300: Train F1:0.663, Train Loss:0.196, Train Acc:91.723%, Val F1:59.798%, Val Loss: 1.089, Val Acc: 69.820%\n",
    "\n",
    "Epoch: 399, Batch: 0, Loss: 0.44639092683792114\n",
    "\n",
    "Test dataset metrics: F1: 0.636, Loss: 0.628, Accuracy: 83.784%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| Model: L6.BS64.LR0.01.EP400 |\n",
      "-------------------------------\n",
      "Epoch 000: Train F1:nan, Train Loss:87.474, Train Acc:48.620%, Val F1:62.282%, Val Loss: 1.005, Val Acc: 58.108%\n",
      "Epoch 100: Train F1:0.697, Train Loss:0.528, Train Acc:66.791%, Val F1:72.677%, Val Loss: 1.104, Val Acc: 53.153%\n",
      "Epoch 200: Train F1:nan, Train Loss:1.100, Train Acc:33.408%, Val F1:66.724%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch 300: Train F1:nan, Train Loss:1.099, Train Acc:32.563%, Val F1:66.244%, Val Loss: 1.099, Val Acc: 33.333%\n",
      "Epoch: 399, Batch: 60, Loss: 1.0995328426361084\n",
      "Test dataset metrics: F1: 0.668, Loss: 1.099, Accuracy: 33.333%\n"
     ]
    }
   ],
   "source": [
    "# best parameters we found\n",
    "n_classes, n_layers, batch_size, learning_rate, n_epochs = 3, 6, 64, 0.01, 400\n",
    "save_plot_fname = \"L{}.BS{}.LR{}.EP{}\".format(n_layers, batch_size, learning_rate, n_epochs)\n",
    "title = \"| Model: {} |\".format(save_plot_fname)\n",
    "print(\"-\"*len(title))\n",
    "print(title)\n",
    "print(\"-\"*len(title))\n",
    "\n",
    "train_dataset = train_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "validate_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "test_dataset = validate_dataset_original.shuffle(buffer_size=50).batch(batch_size)\n",
    "\n",
    "x_classifier, tl, vl, ta, va, tf1, vf1 = train(train_dataset, validate_dataset, learning_rate, batch_size, n_epochs, n_layers, n_classes, save_plot_fname)\n",
    "test(x_classifier, test_dataset, save_plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
